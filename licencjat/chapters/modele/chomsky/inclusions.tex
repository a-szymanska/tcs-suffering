\subsubsection{Języki regularne i bezkontekstowe}

\begin{theorem}
    Zbiór języków regularnych nad alfabetem \( \Sigma \) jest podzbiorem właściwym zbioru języków bezkonstekstowych nad tym samym alfabetem, tzn. \(\reg{\Sigma} \subsetneq  \cfl{\Sigma}\)
\end{theorem}

\begin{proof}
    Dowodzić tej tezy możemy na 2 sposoby:
    \begin{enumerate}
        \item korzystając z faktu, że da się symulować DFA przy pomocy PDA. Jeśli istnieje DFA dla języka, to ten jest regularny i jeśli istnieje PDA, to jest on bezkontekstowy;
        \item stosując indukcję po wyrażeniu regularnym.
    \end{enumerate}
    Istotność zawierania w obu przypadkach wynika z przykładu zaprezentowanego w sekcji \ref{cfl_example}.
    
    Przedstawiony dowód będzie korzystał z drugiego sposobu, czyli indukcji strukturalnej po wyrażeniu regularnym pokazującej, że można na jego podstawie skonstruować gramatykę bezkontekstową. 
    
    Przypadek bazowy: Jeśli mamy wyrażenie regularne \(\alpha\) postaci \(a\) dla pewnego \(a \in \Sigma\), to \(L(\alpha) = a\). Konstrukcja takiej CFG, która rozpoznaje ten język, jest trywialna -- wystarczy, żeby miała produkcję postaci \(S \rightarrow a\). 
    
    Krok indukcyjny - załóżmy, że mamy wyraenie regularne postaci:    
    \begin{enumerate}
        \item \( \alpha_1 + \alpha_2\) -- z założenia indukcyjnego istnieją takie \(G_1\), \(G_2\), że \(L(G_1) = L(\alpha_1)\) i \(L(G_2) = L(\alpha_2)\). Gramatyki \(G_1, G_2\) mają symbole startowe \(S_1, S_2\). Konstruujemy gramatykę \(G_s\) z dwiema produkcjami: 
        \(S \rightarrow S_1\),  \(S \rightarrow S_2\), gdzie \(S\) to symbol startowy gramatyki \(G_s\). Zauważamy, że \(L(G_s) = L(\alpha_1 + \alpha_2)\), czyli konstrukcja jest poprawna.
        
        \item \( \alpha_1 \alpha_2\) -- przy takich samych założeniach jak poprzednio konstruujemy gramatykę \(G_s\) z produkcją \(S \rightarrow S_1S_2\).
        
        \item \( \alpha_1^*\) -- przy takich samych założeniach jak poprzednio konstruujemy gramatykę \(G_s\) z produkcjami: \( S \rightarrow S_1S\), \( S \rightarrow \eps \).
    \end{enumerate}
    To kończy dowód indukcyjny, wskazując konkretną konstrukcję gramatyki bezkontekstowej na podstawie wyrażenia regularnego.
\end{proof}

\subsubsection{Języki bezkontekstowe i kontekstowe}

\begin{theorem}
Zbiór języków bezkontekstowych nad alfabetem \( \Sigma \) jest podzbiorem właściwym zbioru języków kontekstowych nad tym samym alfabetem, tzn. \(\cfl{\Sigma} \subsetneq \csl{\Sigma}\).
\end{theorem}

\begin{proof}
Istotność zawierania wynika z sekcji \ref{csl_example}, gdzie zaprezentowany jest przykład języka kontekstowego, który nie jest bezkontekstowy. Samo zawieranie wynika z faktu, że każda gramatyka bezkontekstowa jest również gramatyką kontekstową, bo każdą produkcję z gramatyki bezkontekstowej postaci: 
\[
    A \rightarrow \beta 
\]
można zapisać w gramatyce kontekstowej jako
\[ 
  \alpha_1 A \alpha_2 \rightarrow \alpha_1 \beta \alpha_2,
\]
gdzie \(\alpha_1 = \alpha_2 = \eps\).
\end{proof}

\subsubsection{Języki kontekstowe i rekurencyjne}
\begin{theorem}
Zbiór języków kontekstowych nad alfabetem \( \Sigma \) jest podzbiorem właściwym zbioru języków rekurencyjnych nad tym samym alfabetem, tzn. \(\csl{\Sigma} \subsetneq \r_{\Sigma}\).
\end{theorem}

\begin{proof}
LBA to w szczególności Maszyna Turinga, ale z ograniczeniem na rozmiar taśmy. Więc jeśli LBA rozpoznaje jakiś język, to MT też go rozpozna\footnote{Zaniepokojonych problemem stopu dla LBA informujemy, że problem stopu dla LBA jest rozstrzygalny; rozstrzygalność jego wynika z prostej rzeczy: mianowicie konfiguracji LBA jest skończenie wiele.}.

Przykład języka, który jest rozpoznawany przez MT a nie przez LBA prezentujemy w sekcji \ref{recursive-example}
\end{proof}

\subsubsection{Języki rekurencyjne i rekurencyjnie przeliczalne}
\begin{theorem}
Zbiór języków rekurencyjnych nad alfabetem \( \Sigma \) jest podzbiorem właściwym zbioru języków rekurencyjnie przeliczalnych nad tym samym alfabetem, tzn. \( \r \subsetneq \re \).
\end{theorem}

\begin{proof}
Każda Maszyna Turinga z własnością stopu jest Maszyną Turinga tak ogólnie. Natomiast język stopu jest przykładem języka który jest rozpoznwany jedynie przez Maszyny Turinga bez własności stopu. Więcej na ten temat jest w sekcji \ref{lhp}.
\end{proof}